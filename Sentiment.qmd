---
title: "Untitled"
format: html
---

```{python}

import pandas as pd

df = pd.read_csv("Climate3.0 - President.csv")

print(df.head())


```

```{python}

from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_name = "climatebert/distilroberta-base-climate-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model.to(device)

sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

df = pd.read_csv("Climate3.0 - President.csv")

def get_sentiment(text):
    if pd.isna(text) or text.strip() == "":
        return "neutral", 0.0  
    result = sentiment_pipeline(text)
    return result[0]['label'], result[0]['score']

df[['Sentiment', 'Score']] = df['Content'].apply(lambda x: pd.Series(get_sentiment(str(x))))

output_file = 'Climate3.0_sentiment_results.csv'
df.to_csv(output_file, index=False)
print(f"Sentiment analysis completed and saved to '{output_file}'")

```

```{python}

sentiment_counts = df['Sentiment'].value_counts()
print("\nSentiment Counts:")
print(sentiment_counts)

risk_count = sentiment_counts.get('risk', 0)
neutral_count = sentiment_counts.get('neutral', 0)
opportunity_count = sentiment_counts.get('opportunity', 0)

print(f"\nNumber of 'risk': {risk_count}")
print(f"Number of 'neutral': {neutral_count}")
print(f"Number of 'opportunity': {opportunity_count}")

```

```{python}


import snscrape.modules.twitter as sntwitter
import pandas as pd

# Declare variables and query
tweets_list = []
tweet_count = 100000
query = "Climate Change since:2021-01-01 until:2022-11-09"

# Use TwitterSearchScraper to scrape data and append tweets to the list
try:
    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
        if i >= tweet_count:
            break
        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])

    # Create a DataFrame from the list of tweets
    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])
    print(f"Number of tweets scraped: {len(tweets_df)}")

    # Display the first few rows
    print(tweets_df.head())

    # Optionally save the DataFrame to a CSV file
    tweets_df.to_csv('climate_tweets.csv', index=False)
    print("Tweets saved to 'climate_tweets.csv'.")

except Exception as e:
    print(f"An error occurred: {e}")

```

```{python}

import tweepy

# Replace these with your actual credentials
api_key = ''
api_secret = 'YOUR_API_SECRET'
access_token = '1854551483146235904-EBtETuKKPXOmHOJlwNV6TaMat1CkrY'
access_token_secret = 'RDcrrt6rCSifxaiNOGhXMxkPDmcoptfRolOBI9k0C0cQm'
bearer_token = 'AAAAAAAAAAAAAAAAAAAAAAjiwwEAAAAAzXc3dzMRa45%2FqunTrNOPHmuEDo0%3Dg0DTVVepuE4sQd5sRkRVMvV1sun2hjyOY41cjfVtLhTKg9qIDN'

# Authenticate with OAuth1 (using access tokens)
auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_token_secret)
api = tweepy.API(auth)

# Verify credentials
try:
    api.verify_credentials()
    print("Authentication successful")
except Exception as e:
    print("Error during authentication:", e)

# Example: Search for recent tweets
query = "Climate Change"
tweets = tweepy.Cursor(api.search_tweets, q=query, lang="en", tweet_mode='extended').items(10)

for tweet in tweets:
    print(f"{tweet.user.name}: {tweet.full_text}\n")

```

```{python}

import tweepy

# Authenticate using the Bearer Token
client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAAjiwwEAAAAAzXc3dzMRa45%2FqunTrNOPHmuEDo0%3Dg0DTVVepuE4sQd5sRkRVMvV1sun2hjyOY41cjfVtLhTKg9qIDN')

# Example: Search recent tweets
query = "Climate Change lang:en"
response = client.search_recent_tweets(query=query, max_results=10)

# Print the results
for tweet in response.data:
    print(tweet.text)

```

```{python}

import tweepy

# Replace with your Bearer Token
bearer_token = ""

# Authenticate with the Bearer Token
client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAAjiwwEAAAAAzXc3dzMRa45%2FqunTrNOPHmuEDo0%3Dg0DTVVepuE4sQd5sRkRVMvV1sun2hjyOY41cjfVtLhTKg9qIDN')

# Example: Search recent tweets with a specific query
query = "Climate Change"
tweets = client.search_recent_tweets(query=query, max_results=10)

for tweet in tweets.data:
    print(f"Tweet ID: {tweet.id}, Text: {tweet.text}")

```

```{python}

import tweepy
import pandas as pd

# Replace this with your Bearer Token from the X API
bearer_token = "AAAAAAAAAAAAAAAAAAAAAAjiwwEAAAAAzXc3dzMRa45%2FqunTrNOPHmuEDo0%3Dg0DTVVepuE4sQd5sRkRVMvV1sun2hjyOY41cjfVtLhTKg9qIDN"

# Authenticate with the Bearer Token
client = tweepy.Client(bearer_token=bearer_token)

# List of Twitter handles for U.S. Senators (you can add more)
senators = [
    "SenatorDurbin", "SenatorLeahy", "SenatorCollins", "SenatorLankford",
    "SenSchumer", "SenatorWicker", "SenatorFeinstein", "SenatorTimScott",
    "SenatorMenendez", "SenatorBaldwin", "tedcruz", "RandPaul"
]

# Define the search query (filtering by "Climate Change")
query = "Climate Change"

# Define an empty list to store tweet data
tweets_list = []

# Loop through each Senator's handle and fetch tweets
for senator in senators:
    # Fetch tweets mentioning 'Climate Change' from each Senator
    try:
        tweets = client.search_recent_tweets(
            query=f"from:{senator} {query}",
            tweet_fields=['created_at', 'public_metrics', 'author_id'],
            max_results=2
        )
        
        # If tweets are found, append to the list
        if tweets.data:
            for tweet in tweets.data:
                tweets_list.append({
                    'Senator': senator,
                    'Tweet ID': tweet.id,
                    'Datetime': tweet.created_at,
                    'Text': tweet.text,
                    'Likes': tweet.public_metrics['like_count'],
                    'Retweets': tweet.public_metrics['retweet_count'],
                    'Replies': tweet.public_metrics['reply_count'],
                    'Views': tweet.public_metrics.get('view_count', 0)
                })
    
    except Exception as e:
        print(f"Error fetching tweets for {senator}: {e}")

# Convert the list to a DataFrame for easier viewing
df = pd.DataFrame(tweets_list)

# Display the DataFrame
print(df)

# Save the DataFrame to a CSV file
df.to_csv('climate_change_tweets_from_senators.csv', index=False)
print("Data saved to 'climate_change_tweets_from_senators.csv'")

```

```{python}



