---
title: "Untitled"
format: html
---

```{python}

import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
import torch
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

```

###

```{python}

df = pd.read_csv("../Data/processed-data/Senators_sentiment.csv")

```

```{python}

model_name = "SamLowe/roberta-base-go_emotions"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

emotion_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    padding=True,          
    truncation=True,       
    max_length=512         
)

def detect_emotion(text):
    if isinstance(text, str) and len(text.strip()) > 0:
        try:
            result = emotion_pipeline(text)
            return result[0]['label'], result[0]['score']
        except IndexError:
            return None, None
    return None, None

df[['emotion', 'confidence']] = df['message'].apply(
    lambda x: pd.Series(detect_emotion(x))
)

output_file = "../Data/processed-data/Senators_sentiment.csv"
df.to_csv(output_file, index=False)

```

```{python}

text_data = df['message'].dropna().astype(str)

vectorizer = CountVectorizer(
    max_df=0.95,  
    min_df=2,    
    stop_words='english'  
)
text_vectors = vectorizer.fit_transform(text_data)

num_topics = 5 
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(text_vectors)

def get_top_words(model, feature_names, n_top_words):
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        topics[topic_idx] = ', '.join(top_words)  
    return topics

n_top_words = 10
feature_names = vectorizer.get_feature_names_out()
topics = get_top_words(lda, feature_names, n_top_words)

print("Discovered Topics:")
for topic_idx, words in topics.items():
    print(f"Topic {topic_idx}: {words}")

topic_probabilities = lda.transform(text_vectors)  
topic_assignments = topic_probabilities.argmax(axis=1) 

topic_labels = [topics[topic_idx] for topic_idx in topic_assignments]

filtered_df = df.loc[df['message'].notna()].copy()
filtered_df['dominant_topic'] = topic_labels  
filtered_df['topic_probability'] = topic_probabilities.max(axis=1)

df = pd.concat([df, filtered_df[['dominant_topic', 'topic_probability']]], axis=1)

output_file = "../Data/processed-data/Senators_sentiment.csv"
df.to_csv(output_file, index=False)

```

## President

```{python}

president_df = pd.read_csv("../Data/processed-data/President_sentiment.csv")

```

```{python}

model_name = "SamLowe/roberta-base-go_emotions"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

emotion_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    padding=True,          
    truncation=True,       
    max_length=512         
)

def detect_emotion(text):
    if isinstance(text, str) and len(text.strip()) > 0:
        try:
            result = emotion_pipeline(text)
            return result[0]['label'], result[0]['score']
        except IndexError:
            return None, None
    return None, None

president_df[['emotion', 'confidence']] = president_df['message'].apply(
    lambda x: pd.Series(detect_emotion(x))
)

output_file = "../Data/processed-data/President_sentiment.csv"
president_df.to_csv(output_file, index=False)

```

```{python}

text_data = president_df['message'].dropna().astype(str)

vectorizer = CountVectorizer(
    max_df=0.95,  
    min_df=2,    
    stop_words='english'  
)
text_vectors = vectorizer.fit_transform(text_data)

num_topics = 5 
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(text_vectors)

def get_top_words(model, feature_names, n_top_words):
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        topics[topic_idx] = ', '.join(top_words)  
    return topics

n_top_words = 10
feature_names = vectorizer.get_feature_names_out()
topics = get_top_words(lda, feature_names, n_top_words)

print("Discovered Topics:")
for topic_idx, words in topics.items():
    print(f"Topic {topic_idx}: {words}")

topic_probabilities = lda.transform(text_vectors)  
topic_assignments = topic_probabilities.argmax(axis=1) 

topic_labels = [topics[topic_idx] for topic_idx in topic_assignments]

filtered_df = president_df.loc[president_df['message'].notna()].copy()
filtered_df['dominant_topic'] = topic_labels  
filtered_df['topic_probability'] = topic_probabilities.max(axis=1)

president_df = pd.concat([df, filtered_df[['dominant_topic', 'topic_probability']]], axis=1)

output_file = "../Data/processed-data/President_sentiment.csv"
president_df.to_csv(output_file, index=False)

```

