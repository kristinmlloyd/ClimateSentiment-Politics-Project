{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/raw-data/train.csv\")\n",
    "test_df = pd.read_csv(\"../Data/raw-data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', \"\", text)\n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', cleanText)\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText)\n",
    "    cleanText = re.sub(r'RT', '', cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    cleanText = re.sub(r'([https][http][htt][th][ht])', \"\", cleanText)\n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise])\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128)\n",
    "    cleanText = tokenizer.tokenize(cleanText)\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list]\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].apply(preprocess)\n",
    "test_df['message'] = test_df['message'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(ngram_range=(1,20), min_df=2)\n",
    "train_features = vector.fit_transform(train_df['message'])\n",
    "test_features = vector.transform(test_df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, \n",
    "    train_df['sentiment'],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE...\n",
      "Before SMOTE: Counter({1: 163, -1: 109})\n",
      "After SMOTE: Counter({-1: 163, 1: 163})\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying SMOTE...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Before SMOTE:', Counter(y_train))\n",
    "print('After SMOTE:', Counter(y_train_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "LogisticRegression - Validation Accuracy: 0.9130, F1: 0.9126\n",
      "Training ForestClassifier...\n",
      "ForestClassifier - Validation Accuracy: 0.8406, F1: 0.8315\n",
      "Training NaiveBayes...\n",
      "NaiveBayes - Validation Accuracy: 0.9130, F1: 0.9126\n",
      "Training LinearSVM...\n",
      "LinearSVM - Validation Accuracy: 0.9420, F1: 0.9419\n",
      "Training KNNClassifier...\n",
      "KNNClassifier - Validation Accuracy: 0.4783, F1: 0.3463\n"
     ]
    }
   ],
   "source": [
    "names = ['LogisticRegression', 'ForestClassifier', 'NaiveBayes', 'LinearSVM', 'KNNClassifier']\n",
    "classifiers = [\n",
    "    LogisticRegression(C=10),\n",
    "    RandomForestClassifier(criterion='entropy'),\n",
    "    MultinomialNB(alpha=1),\n",
    "    LinearSVC(C=10, class_weight=None),\n",
    "    KNeighborsClassifier(n_neighbors=10)\n",
    "]\n",
    "\n",
    "results = []\n",
    "models = {}\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(f'Training {name}...')\n",
    "    \n",
    "    clf.fit(X_train_sm, y_train_sm)\n",
    "    \n",
    "    val_pred = clf.predict(X_val)\n",
    "    test_pred = clf.predict(test_features)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    models[name] = clf\n",
    "    results.append([name, val_accuracy, val_f1])\n",
    "    \n",
    "    test_df[f'{name}_predictions'] = test_pred\n",
    "    \n",
    "    print(f'{name} - Validation Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Validation Set:\n",
      "                    Validation Accuracy  Validation F1\n",
      "Classifier                                            \n",
      "LinearSVM                      0.942029       0.941919\n",
      "LogisticRegression             0.913043       0.912584\n",
      "NaiveBayes                     0.913043       0.912584\n",
      "ForestClassifier               0.840580       0.831521\n",
      "KNNClassifier                  0.478261       0.346316\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Classifier', 'Validation Accuracy', 'Validation F1'])\n",
    "results_df.set_index('Classifier', inplace=True)\n",
    "\n",
    "print(\"\\nModel Performance on Validation Set:\")\n",
    "print(results_df.sort_values('Validation F1', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"model_predictions.csv\"\n",
    "test_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nPredictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Dataset:\n",
      "\n",
      "LogisticRegression:\n",
      "Accuracy: 0.8455\n",
      "F1 Score: 0.8430\n",
      "\n",
      "ForestClassifier:\n",
      "Accuracy: 0.7077\n",
      "F1 Score: 0.7076\n",
      "\n",
      "NaiveBayes:\n",
      "Accuracy: 0.8455\n",
      "F1 Score: 0.8405\n",
      "\n",
      "LinearSVM:\n",
      "Accuracy: 0.8309\n",
      "F1 Score: 0.8278\n",
      "\n",
      "KNNClassifier:\n",
      "Accuracy: 0.6221\n",
      "F1 Score: 0.3937\n"
     ]
    }
   ],
   "source": [
    "if 'sentiment' in test_df.columns:\n",
    "    print(\"\\nModel Performance on Dataset:\")\n",
    "    for name in names:\n",
    "        predictions = test_df[f'{name}_predictions']\n",
    "        accuracy = accuracy_score(test_df['sentiment'], predictions)\n",
    "        f1 = f1_score(test_df['sentiment'], predictions, average='macro')\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of final model labels:\n",
      "model_label\n",
      "-1    244\n",
      " 1    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"model_predictions.csv\")\n",
    "\n",
    "selected_columns = ['LogisticRegression_predictions', 'NaiveBayes_predictions', 'LinearSVM_predictions']\n",
    "df['model_label'] = df[selected_columns].mode(axis=1).iloc[:,0]\n",
    "\n",
    "for col in df.columns:\n",
    "    if '_predictions' in col:\n",
    "        df = df.drop(col, axis=1)\n",
    "\n",
    "df.to_csv(\"../Data/processed-data/model_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nDistribution of final model labels:\")\n",
    "print(df['model_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/processed-data/model_predictions.csv\")\n",
    "\n",
    "president_df = df[df['state'] == 'US']\n",
    "senators_df = df[df['state'] != 'US']\n",
    "\n",
    "president_df.to_csv(\"../Data/processed-data/President_sentiment.csv\", index=False)\n",
    "senators_df.to_csv(\"../Data/processed-data/Senators_sentiment.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
