{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Sentiment Model\"\n",
    "author: \"Kristin Lloyd\"\n",
    "format: \n",
    "  html:\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start by importing codecarbon to keep track of my machine learning model's carbon emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 21:10:29] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 21:10:29] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:10:29] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:10:29] No GPU found.\n",
      "[codecarbon INFO @ 21:10:29] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:10:29] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Mac OS and ARM processor detected: Please enable PowerMetrics sudo to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 21:10:30] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 21:10:30] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 21:10:30] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:10:30]   Platform system: macOS-14.6-arm64-arm-64bit\n",
      "[codecarbon INFO @ 21:10:30]   Python version: 3.11.8\n",
      "[codecarbon INFO @ 21:10:30]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 21:10:30]   Available RAM : 16.000 GB\n",
      "[codecarbon INFO @ 21:10:30]   CPU count: 8\n",
      "[codecarbon INFO @ 21:10:30]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 21:10:30]   GPU count: None\n",
      "[codecarbon INFO @ 21:10:30]   GPU model: None\n",
      "[codecarbon INFO @ 21:10:32] Saving emissions data to file /Users/kristinlloyd/Downloads/ClimateSentiment-Politics-Project/Sentiment/emissions.csv\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker(allow_multiple_runs=True)\n",
    "\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is adapted from, \n",
    "https://github.com/MichaelOmosebi/Sentiments-Analysis-Climate-Change. The first time I run through this code, I am only going to make minor changes to the code and change the test.csv and train.csv to match my own. The second time I run through this code, I am going to make my own changes to the model and play around with the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My train_df dataset consists of fake tweets generated by ChatGPT. Initially, I attempted to use a dataset from GitHub containing real tweets about climate change, but the accuracy scores were only around 50%. I believe using ChatGPT generated tweets improved the model's accuracy because the dataset was cleaner and I prompted it to capture emotions such as sarcasm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we load and process the training and test datasets. The code removes URLs, mentions, hashtags, punctuation, digits, and stopwords. It also lemmatizes words and tokenizes the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/raw-data/train.csv\")\n",
    "test_df = pd.read_csv(\"../Data/raw-data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:10:47] Energy consumed for RAM : 0.000025 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:10:47] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:10:47] 0.000202 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', \"\", text)\n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', cleanText)\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText)\n",
    "    cleanText = re.sub(r'RT', '', cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    cleanText = re.sub(r'([https][http][htt][th][ht])', \"\", cleanText)\n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise])\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128)\n",
    "    cleanText = tokenizer.tokenize(cleanText)\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list]\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].apply(preprocess)\n",
    "test_df['message'] = test_df['message'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer converts text data into numerical features. Min_df = 2 means that it will ignore phrases that appear in less than two messages. Ngram_range = (1,20) means that it will look at one word up to 20 consecutive words. This lets the vectorizer capture phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(ngram_range=(1,20), min_df=2)\n",
    "train_features = vector.fit_transform(train_df['message'])\n",
    "test_features = vector.transform(test_df['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below splits the data into training and validation sets. In this code, we are doing an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, \n",
    "    train_df['sentiment'],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE is used incase there is an imbalance between opportunistic and risky sentiment. The purpose of SMOTE is to create a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE...\n",
      "Before SMOTE: Counter({1: 163, -1: 109})\n",
      "After SMOTE: Counter({-1: 163, 1: 163})\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying SMOTE...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Before SMOTE:', Counter(y_train))\n",
    "print('After SMOTE:', Counter(y_train_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models used are LogisticRegression, RandomForestClassifier, NaiveBayes, LinearSVM, and KNNClassifier. LogisticRegression predicts whether something belongs to one category or another based on patterns in the data, RandomForest uses decision trees to make predictions, LinearSVM finds the best boundary to separate categories, and KNN looks at nearest neighbors to choose categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "LogisticRegression - Validation Accuracy: 0.9130, F1: 0.9126\n",
      "Training ForestClassifier...\n",
      "ForestClassifier - Validation Accuracy: 0.8696, F1: 0.8640\n",
      "Training NaiveBayes...\n",
      "NaiveBayes - Validation Accuracy: 0.9130, F1: 0.9126\n",
      "Training LinearSVM...\n",
      "LinearSVM - Validation Accuracy: 0.9420, F1: 0.9419\n",
      "Training KNNClassifier...\n",
      "KNNClassifier - Validation Accuracy: 0.4783, F1: 0.3463\n"
     ]
    }
   ],
   "source": [
    "names = ['LogisticRegression', 'ForestClassifier', 'NaiveBayes', 'LinearSVM', 'KNNClassifier']\n",
    "classifiers = [\n",
    "    LogisticRegression(C=10),\n",
    "    RandomForestClassifier(criterion='entropy'),\n",
    "    MultinomialNB(alpha=1),\n",
    "    LinearSVC(C=10, class_weight=None),\n",
    "    KNeighborsClassifier(n_neighbors=10)\n",
    "]\n",
    "\n",
    "results = []\n",
    "models = {}\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(f'Training {name}...')\n",
    "    \n",
    "    clf.fit(X_train_sm, y_train_sm)\n",
    "    \n",
    "    val_pred = clf.predict(X_val)\n",
    "    test_pred = clf.predict(test_features)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    models[name] = clf\n",
    "    results.append([name, val_accuracy, val_f1])\n",
    "    \n",
    "    test_df[f'{name}_predictions'] = test_pred\n",
    "    \n",
    "    print(f'{name} - Validation Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Validation Set:\n",
      "                    Validation Accuracy  Validation F1\n",
      "Classifier                                            \n",
      "LinearSVM                      0.942029       0.941919\n",
      "LogisticRegression             0.913043       0.912584\n",
      "NaiveBayes                     0.913043       0.912584\n",
      "ForestClassifier               0.869565       0.863965\n",
      "KNNClassifier                  0.478261       0.346316\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Classifier', 'Validation Accuracy', 'Validation F1'])\n",
    "results_df.set_index('Classifier', inplace=True)\n",
    "\n",
    "print(\"\\nModel Performance on Validation Set:\")\n",
    "print(results_df.sort_values('Validation F1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVM performed the best with 94.2% accuracy, followed by LogisticRegression and NaiveBayes at 91.3% accuracy, followed by ForestClassifier at 86.9% accuracy, followed by the worst performer -- KNNClassifier at a low 47.8% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to model_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:11:02] Energy consumed for RAM : 0.000050 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:11:02] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:11:02] 0.000404 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"model_predictions.csv\"\n",
    "test_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nPredictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Dataset:\n",
      "\n",
      "LogisticRegression:\n",
      "Accuracy: 0.8455\n",
      "F1 Score: 0.8430\n",
      "\n",
      "ForestClassifier:\n",
      "Accuracy: 0.6701\n",
      "F1 Score: 0.6697\n",
      "\n",
      "NaiveBayes:\n",
      "Accuracy: 0.8455\n",
      "F1 Score: 0.8405\n",
      "\n",
      "LinearSVM:\n",
      "Accuracy: 0.8309\n",
      "F1 Score: 0.8278\n",
      "\n",
      "KNNClassifier:\n",
      "Accuracy: 0.6221\n",
      "F1 Score: 0.3937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:11:17] Energy consumed for RAM : 0.000075 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:11:17] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:11:17] 0.000606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:11:32] Energy consumed for RAM : 0.000100 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:11:32] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:11:32] 0.000809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:11:47] Energy consumed for RAM : 0.000125 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:11:47] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:11:47] 0.001011 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "if 'sentiment' in test_df.columns:\n",
    "    print(\"\\nModel Performance on Dataset:\")\n",
    "    for name in names:\n",
    "        predictions = test_df[f'{name}_predictions']\n",
    "        accuracy = accuracy_score(test_df['sentiment'], predictions)\n",
    "        f1 = f1_score(test_df['sentiment'], predictions, average='macro')\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test dataset, LogisticRegression and NaiveBayes performed the best with 84.55% accuracy, followed by LinearSVM at 83% accuracy, followed by ForestClassifier at 67% accuracy, followed by the worst performer -- KNNClassifier at 62% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of final model labels:\n",
      "model_label\n",
      "-1    244\n",
      " 1    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"model_predictions.csv\")\n",
    "\n",
    "selected_columns = ['LogisticRegression_predictions', 'NaiveBayes_predictions', 'LinearSVM_predictions']\n",
    "df['model_label'] = df[selected_columns].mode(axis=1).iloc[:,0]\n",
    "\n",
    "for col in df.columns:\n",
    "    if '_predictions' in col:\n",
    "        df = df.drop(col, axis=1)\n",
    "\n",
    "df.to_csv(\"../Data/processed-data/model_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nDistribution of final model labels:\")\n",
    "print(df['model_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Additions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../Data/raw-data/train.csv\")\n",
    "test_df = pd.read_csv(\"../Data/raw-data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a few very minor changes in preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = set(stopwords.words('english'))  # Convert to set for faster lookup\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+', \"\", text)\n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', cleanText)\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText)\n",
    "    cleanText = re.sub(r'RT', '', cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    \n",
    "    cleanText = re.sub(r'[^\\x00-\\x7F]+', '', cleanText)  # Removed emojis\n",
    "    cleanText = re.sub(r\"'s\\b\", \"\", cleanText)  # Removed possessive\n",
    "    cleanText = re.sub(r\"n't\\b\", \" not\", cleanText)  # Handled negations\n",
    "    cleanText = re.sub(r'([https][http][htt][th][ht])', \"\", cleanText)\n",
    "    \n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise])\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128)\n",
    "    cleanText = tokenizer.tokenize(cleanText)\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list]\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Preprocessing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:12:02] Energy consumed for RAM : 0.000150 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:12:02] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:12:02] 0.001213 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing training data...\")\n",
    "train_df['message'] = train_df['message'].apply(preprocess)\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['message'] = test_df['message'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I changed the ngram_range from (1,20) to (1,3). I did this for more efficient processing and to reduce noise/overfitting. (1,3) is more practical, especially because my dataset is a lot smaller. I also added max_df = 0.95, which removes words that appear in more than 95% of the document. Likely, the words \"climate\" and \"change\" will dissapear. This is beneficial because everyone in the dataset is talking about climate change so these terms do not help determine sentiment. I also added max_features = 5000. Even though the datasets are small, it is important to set a cap. I also added token_pattern = r'\\b\\w+\\b' for properly separating words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved TF-IDF vectorization\n",
    "vector = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    max_features=5000,\n",
    "    token_pattern=r'\\b\\w+\\b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Applying SMOTE...\n",
      "Before SMOTE: Counter({1: 163, -1: 109})\n",
      "After SMOTE: Counter({-1: 163, 1: 163})\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features...\")\n",
    "train_features = vector.fit_transform(train_df['message'])\n",
    "test_features = vector.transform(test_df['message'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_features, \n",
    "    train_df['sentiment'],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Applying SMOTE...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Before SMOTE:', Counter(y_train))\n",
    "print('After SMOTE:', Counter(y_train_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Logistic Regression and Linear SVC, I changed C from 10 to 1 to prevent overfitting. Adding a class weight made sure the models pay equal attention to opportunistic and risky tweets, even though the dataset has more opportunistic tweets. I also increased the training iterations. Changing C in LinearSVC helped but after playing around with the C in LogisticRegression, I decided to change it back to 10. \n",
    "\n",
    "For the Random Forest model, I added 200 more trees to help the model reach a conclusion and added min_samples_split = 5 to prevent the model from overfitting.\n",
    "\n",
    "For Naive Bayes, I changed alpha to equal 0.5 because this makes it more flexible in learning pattterns. \n",
    "\n",
    "For K-Nearest Neighbors, I reuced the number of neighbors to 5. This way, the model will only look at the most similar content. I also added the cosine metric because it is generaly good with text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['LogisticRegression', 'ForestClassifier', 'NaiveBayes', 'LinearSVM', 'KNNClassifier']\n",
    "classifiers = [\n",
    "    LogisticRegression(C=10, class_weight='balanced', max_iter=1000),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        criterion='entropy',\n",
    "        max_depth=None,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    MultinomialNB(alpha=0.5),\n",
    "    LinearSVC(C=1.0, class_weight='balanced', max_iter=2000),\n",
    "    KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        weights='distance',\n",
    "        metric='cosine'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I reorganized the training and evaluation code into a function called evaluate_model. My goal was to make the code cleaner and easier to reuse/modify later. I also added cross validation, because it is a very reliable way to test how well the model works. Cross validation splits the data into 5 parts and uses each as the test set once. This tells us if our model is getting lucky or if it is performing similarly across all splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation function\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def evaluate_model(name, clf, X_train, y_train, X_val, y_val, X_test, test_df):\n",
    "    print(f'\\nTraining {name}...')\n",
    "    \n",
    "    # Added Cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    print(f\"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    val_pred = clf.predict(X_val)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    test_df[f'{name}_predictions'] = test_pred\n",
    "    \n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')\n",
    "    \n",
    "    return val_accuracy, val_f1, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "CV Score: 0.9694 (+/- 0.0433)\n",
      "Validation Accuracy: 0.9275, F1: 0.9273\n",
      "\n",
      "Training ForestClassifier...\n",
      "CV Score: 0.9203 (+/- 0.1054)\n",
      "Validation Accuracy: 0.8696, F1: 0.8640\n",
      "\n",
      "Training NaiveBayes...\n",
      "CV Score: 0.9755 (+/- 0.0460)\n",
      "Validation Accuracy: 0.9275, F1: 0.9270\n",
      "\n",
      "Training LinearSVM...\n",
      "CV Score: 0.9724 (+/- 0.0408)\n",
      "Validation Accuracy: 0.9275, F1: 0.9273\n",
      "\n",
      "Training KNNClassifier...\n",
      "CV Score: 0.9632 (+/- 0.0499)\n",
      "Validation Accuracy: 0.9130, F1: 0.9126\n",
      "\n",
      "Model Performance on Validation Set:\n",
      "                    Validation Accuracy  Validation F1\n",
      "Classifier                                            \n",
      "LogisticRegression             0.927536       0.927292\n",
      "LinearSVM                      0.927536       0.927292\n",
      "NaiveBayes                     0.927536       0.926984\n",
      "KNNClassifier                  0.913043       0.912584\n",
      "ForestClassifier               0.869565       0.863965\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "models = {}\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    val_accuracy, val_f1, test_pred = evaluate_model(\n",
    "        name, clf, X_train_sm, y_train_sm, \n",
    "        X_val, y_val, test_features, test_df\n",
    "    )\n",
    "    models[name] = clf\n",
    "    results.append([name, val_accuracy, val_f1])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Classifier', 'Validation Accuracy', 'Validation F1'])\n",
    "results_df.set_index('Classifier', inplace=True)\n",
    "\n",
    "print(\"\\nModel Performance on Validation Set:\")\n",
    "print(results_df.sort_values('Validation F1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs very consistently and does a good job, along with NaiveBayes and LinearSVM. ForestClassifier is 93% accurate in cross validation but drops to 88% on the validation set. KNN also does a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I decided to add in precision and recall scores, confusion matrices, and a classification report for each model. I also organized these results into a DataFrame to easily compare how different models perform across all metrics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Training ForestClassifier...\n",
      "Training NaiveBayes...\n",
      "Training LinearSVM...\n",
      "Training KNNClassifier...\n",
      "\n",
      "Model Performance on Test Set:\n",
      "\n",
      "LogisticRegression:\n",
      "Accuracy: 0.8434\n",
      "F1 Score: 0.8411\n",
      "Precision: 0.8432\n",
      "Recall: 0.8642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[231  66]\n",
      " [  9 173]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.78      0.86       297\n",
      "           1       0.72      0.95      0.82       182\n",
      "\n",
      "    accuracy                           0.84       479\n",
      "   macro avg       0.84      0.86      0.84       479\n",
      "weighted avg       0.87      0.84      0.85       479\n",
      "\n",
      "\n",
      "ForestClassifier:\n",
      "Accuracy: 0.7077\n",
      "F1 Score: 0.7076\n",
      "Precision: 0.7612\n",
      "Recall: 0.7558\n",
      "\n",
      "Confusion Matrix:\n",
      "[[165 132]\n",
      " [  8 174]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.56      0.70       297\n",
      "           1       0.57      0.96      0.71       182\n",
      "\n",
      "    accuracy                           0.71       479\n",
      "   macro avg       0.76      0.76      0.71       479\n",
      "weighted avg       0.81      0.71      0.71       479\n",
      "\n",
      "\n",
      "NaiveBayes:\n",
      "Accuracy: 0.8518\n",
      "F1 Score: 0.8475\n",
      "Precision: 0.8435\n",
      "Recall: 0.8613\n",
      "\n",
      "Confusion Matrix:\n",
      "[[244  53]\n",
      " [ 18 164]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.82      0.87       297\n",
      "           1       0.76      0.90      0.82       182\n",
      "\n",
      "    accuracy                           0.85       479\n",
      "   macro avg       0.84      0.86      0.85       479\n",
      "weighted avg       0.86      0.85      0.85       479\n",
      "\n",
      "\n",
      "LinearSVM:\n",
      "Accuracy: 0.8413\n",
      "F1 Score: 0.8385\n",
      "Precision: 0.8388\n",
      "Recall: 0.8593\n",
      "\n",
      "Confusion Matrix:\n",
      "[[233  64]\n",
      " [ 12 170]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.78      0.86       297\n",
      "           1       0.73      0.93      0.82       182\n",
      "\n",
      "    accuracy                           0.84       479\n",
      "   macro avg       0.84      0.86      0.84       479\n",
      "weighted avg       0.87      0.84      0.84       479\n",
      "\n",
      "\n",
      "KNNClassifier:\n",
      "Accuracy: 0.7704\n",
      "F1 Score: 0.7583\n",
      "Precision: 0.7565\n",
      "Recall: 0.7606\n",
      "\n",
      "Confusion Matrix:\n",
      "[[238  59]\n",
      " [ 51 131]]\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.80      0.81       297\n",
      "           1       0.69      0.72      0.70       182\n",
      "\n",
      "    accuracy                           0.77       479\n",
      "   macro avg       0.76      0.76      0.76       479\n",
      "weighted avg       0.77      0.77      0.77       479\n",
      "\n",
      "\n",
      "Overall Test Set Performance Summary:\n",
      "                    Test Accuracy   Test F1  Test Precision  Test Recall\n",
      "Classifier                                                              \n",
      "NaiveBayes               0.851775  0.847521        0.843529     0.861324\n",
      "LogisticRegression       0.843424  0.841094        0.843175     0.864164\n",
      "LinearSVM                0.841336  0.838543        0.838758     0.859289\n",
      "KNNClassifier            0.770355  0.758294        0.756502     0.760564\n",
      "ForestClassifier         0.707724  0.707621        0.761192     0.755800\n"
     ]
    }
   ],
   "source": [
    "for name, clf in zip(names, classifiers):\n",
    "    print(f'Training {name}...')\n",
    "    \n",
    "    clf.fit(X_train_sm, y_train_sm) \n",
    "    test_pred = clf.predict(test_features)\n",
    "    test_df[f'{name}_predictions'] = test_pred\n",
    "\n",
    "def calculate_test_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "test_results = []\n",
    "\n",
    "for name in names:\n",
    "    if 'sentiment' in test_df.columns:\n",
    "        predictions = test_df[f'{name}_predictions']\n",
    "        metrics = calculate_test_metrics(test_df['sentiment'], predictions)\n",
    "        \n",
    "        test_results.append([\n",
    "            name,\n",
    "            metrics['accuracy'],\n",
    "            metrics['f1'],\n",
    "            metrics['precision'],\n",
    "            metrics['recall']\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(test_df['sentiment'], predictions))\n",
    "        \n",
    "        print(\"\\nDetailed Classification Report:\")\n",
    "        print(classification_report(test_df['sentiment'], predictions))\n",
    "\n",
    "test_results_df = pd.DataFrame(\n",
    "    test_results, \n",
    "    columns=['Classifier', 'Test Accuracy', 'Test F1', 'Test Precision', 'Test Recall']\n",
    ")\n",
    "test_results_df.set_index('Classifier', inplace=True)\n",
    "\n",
    "print(\"\\nOverall Test Set Performance Summary:\")\n",
    "print(test_results_df.sort_values('Test F1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN and ForestClassifier saw a significant increase in accuracy. NaiveBayes and LinearSVM went up slightly. LogisticRegression went down very slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of final model labels:\n",
      "model_label\n",
      "-1    244\n",
      " 1    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['LogisticRegression_predictions', 'NaiveBayes_predictions', 'LinearSVM_predictions']\n",
    "\n",
    "test_df['model_label'] = test_df[selected_columns].mode(axis=1).iloc[:,0]\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if '_predictions' in col:\n",
    "        test_df = test_df.drop(col, axis=1)\n",
    "\n",
    "test_df.to_csv(\"../Data/processed-data/model_predictions.csv\", index=False)\n",
    "\n",
    "president_df = test_df[test_df['state'] == 'US']\n",
    "senators_df = test_df[test_df['state'] != 'US']\n",
    "\n",
    "president_df.to_csv(\"../Data/processed-data/President_sentiment.csv\", index=False)\n",
    "senators_df.to_csv(\"../Data/processed-data/Senators_sentiment.csv\", index=False)\n",
    "\n",
    "print(\"\\nDistribution of final model labels:\")\n",
    "print(test_df['model_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 21:25:32] Tracker already stopped !\n",
      "[codecarbon WARNING @ 21:25:32] Background scheduler didn't run for a long period (596s), results might be inaccurate\n",
      "[codecarbon INFO @ 21:25:32] Energy consumed for RAM : 0.001501 kWh. RAM Power : 6.0 W\n",
      "[codecarbon INFO @ 21:25:32] Energy consumed for all CPUs : 0.010631 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:25:32] 0.012132 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CO2 emissions: 0.0045 kg\n"
     ]
    }
   ],
   "source": [
    "emissions = tracker.stop()\n",
    "print(f\"Total CO2 emissions: {emissions:.4f} kg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
